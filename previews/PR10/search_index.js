var documenterSearchIndex = {"docs":
[{"location":"fitting/#Fitting-Methods","page":"Overview","title":"Fitting Methods","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"This page describes the three main fitting methods implemented in StructuredGaussianMixtures.jl: EM, PCAEM, and FactorEM.","category":"page"},{"location":"fitting/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"All fitting methods implement the GMMFitMethod interface and can be used with the fit function:","category":"page"},{"location":"fitting/","page":"Overview","title":"Overview","text":"gmm = fit(fitmethod, data)","category":"page"},{"location":"fitting/#GMMFitMethod-Interface","page":"Overview","title":"GMMFitMethod Interface","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.GMMFitMethod","page":"Overview","title":"StructuredGaussianMixtures.GMMFitMethod","text":"GMMFitMethod\n\nAbstract type for Gaussian Mixture Model fitting methods.\n\n\n\n\n\n","category":"type"},{"location":"fitting/#EM:-Standard-Expectation-Maximization","page":"Overview","title":"EM: Standard Expectation Maximization","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"The EM method fits Gaussian Mixture Models with full covariance matrices using standard Expectation Maximization.","category":"page"},{"location":"fitting/#Constructor","page":"Overview","title":"Constructor","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.EM","page":"Overview","title":"StructuredGaussianMixtures.EM","text":"EM\n\nStandard Expectation Maximization for fitting Gaussian Mixture Models.\n\nFields\n\nn_components: Number of mixture components\nmethod: Initialization method (:kmeans, :rand, etc.)\nkind: Covariance structure (:full, :diag, etc.)\nnInit: Number of initializations\nnIter: Maximum number of iterations\nnFinal: Number of final iterations\n\n\n\n\n\n","category":"type"},{"location":"fitting/#Usage","page":"Overview","title":"Usage","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"using StructuredGaussianMixtures\n\n# Basic usage\nfitmethod = EM(3)\ngmm = fit(fitmethod, data)\n\n# With custom parameters\nfitmethod = EM(5; method=:rand, kind=:full, nInit=100, nIter=20)\ngmm = fit(fitmethod, data)","category":"page"},{"location":"fitting/#Fit-Method","page":"Overview","title":"Fit Method","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{EM, Matrix}","page":"Overview","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::EM, x::Matrix)\n\nFit a Gaussian Mixture Model using Expectation Maximization.\n\nArguments\n\nfitmethod: The EM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\n\nReturns\n\nA MixtureModel of MvNormal distributions\n\nNotes\n\nUses GaussianMixtures.jl's GMM implementation\nSupports different initialization methods and covariance structures\n\n\n\n\n\n","category":"method"},{"location":"fitting/#PCAEM:-Mixture-of-Probabilistic-Principal-Component-Analysis","page":"Overview","title":"PCAEM: Mixture of Probabilistic Principal Component Analysis","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"PCAEM fits a GMM in PCA-reduced space and transforms back to the original space, effectively learning low-rank covariance structures.","category":"page"},{"location":"fitting/#Constructor-2","page":"Overview","title":"Constructor","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.PCAEM","page":"Overview","title":"StructuredGaussianMixtures.PCAEM","text":"PCAEM\n\nMixture of Probabilistic Principal Component Analysis model. Fits a GMM in PCA-reduced space and transforms back to original space.\n\nFields\n\nn_components: Number of mixture components\nrank: Number of principal components to use\ngmm_method: Initialization method for GMM (:kmeans, :rand, etc.)\ngmm_kind: Covariance structure for GMM (:full, :diag, etc.)\ngmm_nInit: Number of GMM initializations\ngmm_nIter: Maximum number of GMM iterations\ngmm_nFinal: Number of final GMM iterations\n\n\n\n\n\n","category":"type"},{"location":"fitting/#Usage-2","page":"Overview","title":"Usage","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"# Basic usage\nfitmethod = PCAEM(3, 2)\ngmm = fit(fitmethod, data)\n\n# With custom parameters\nfitmethod = PCAEM(5, 3; gmm_method=:rand, gmm_nInit=100)\ngmm = fit(fitmethod, data)","category":"page"},{"location":"fitting/#Fit-Method-2","page":"Overview","title":"Fit Method","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{PCAEM, Matrix}","page":"Overview","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::PCAEM, x::Matrix)\n\nFit a Mixture of Probabilistic Principal Component Analysis model. This method first performs PCA to reduce dimensionality, then fits a GMM in the reduced space, and finally transforms the components back to the original space as LRDMvNormal distributions.\n\nArguments\n\nfitmethod: The PCAEM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\n\nReturns\n\nA MixtureModel of LRDMvNormal distributions\n\nNotes\n\nUses PCA for dimensionality reduction\nFits GMM in the reduced space\nTransforms components back to original space with low-rank plus diagonal structure\nThe diagonal noise term is estimated from PCA reconstruction error\n\n\n\n\n\n","category":"method"},{"location":"fitting/#FactorEM:-Mixture-of-Factor-Analyzers","page":"Overview","title":"FactorEM: Mixture of Factor Analyzers","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"FactorEM directly fits GMMs with covariance matrices constrained to the form Σ = FF' + D, where F is a low-rank factor matrix and D is diagonal.","category":"page"},{"location":"fitting/#Constructor-3","page":"Overview","title":"Constructor","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.FactorEM","page":"Overview","title":"StructuredGaussianMixtures.FactorEM","text":"FactorEM\n\nMixture of Factor Analyzers model. Directly fits a mixture of low-rank plus diagonal Gaussian distributions.\n\nFields\n\nn_components: Number of mixture components\nrank: Rank of the low-rank factor\ngmm_method: Initialization method for GMM (:kmeans, :rand, etc.)\ngmm_nInit: Number of GMM initializations\ngmm_nIter: Maximum number of GMM iterations\ngmm_nFinal: Number of final GMM iterations\n\n\n\n\n\n","category":"type"},{"location":"fitting/#Usage-3","page":"Overview","title":"Usage","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"# Basic usage\nfitmethod = FactorEM(3, 2)\ngmm = fit(fitmethod, data)\n\n# With custom parameters\nfitmethod = FactorEM(5, 3; initialization_method=:rand, nInit=10, nIter=20)\ngmm = fit(fitmethod, data)\n\n# With weighted data\nweights = ones(size(data, 2))  # Equal weights\ngmm = fit(fitmethod, data, weights)","category":"page"},{"location":"fitting/#Fit-Methods","page":"Overview","title":"Fit Methods","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{FactorEM, Matrix}","page":"Overview","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::FactorEM, x::Matrix)\n\nFit a Mixture of Factor Analyzers model using Expectation Maximization. This method directly fits a mixture of low-rank plus diagonal Gaussian distributions.\n\nArguments\n\nfitmethod: The FactorEM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\n\nReturns\n\nA MixtureModel of LRDMvNormal distributions\n\nNotes\n\nDirectly fits the low-rank plus diagonal structure\nMore computationally intensive than PCAEM but potentially more accurate\nNot yet implemented\n\n\n\n\n\n","category":"method"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{FactorEM, Matrix, Vector}","page":"Overview","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::FactorEM, x::Matrix, weights::Vector)\n\nFit a Mixture of Factor Analyzers model using Expectation Maximization with weighted data points.\n\nArguments\n\nfitmethod: The FactorEM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\nweights: Vector of weights for each data point\n\nReturns\n\nA MixtureModel of LRDMvNormal distributions\n\nNotes\n\nSupports weighted data points for importance sampling or missing data scenarios\nWeights are automatically normalized to sum to 1\nUses the same EM algorithm but with weighted responsibilities\n\n\n\n\n\n","category":"method"},{"location":"fitting/#Method-Comparison","page":"Overview","title":"Method Comparison","text":"","category":"section"},{"location":"fitting/#When-to-Use-Each-Method","page":"Overview","title":"When to Use Each Method","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"Method Best For Covariance Structure Computational Cost\nEM Low-dimensional data, full covariance needed Full O(m³)\nPCAEM High-dimensional data, dimensionality reduction Low-rank + diagonal O(r³) where r < m\nFactorEM High-dimensional data, direct low-rank fitting Low-rank + diagonal O(r³) where r < m","category":"page"},{"location":"fitting/#Simple-Examples","page":"Overview","title":"Simple Examples","text":"","category":"section"},{"location":"fitting/#Basic-Fitting","page":"Overview","title":"Basic Fitting","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"using StructuredGaussianMixtures\n\n# Generate some data\ndata = randn(2, 1000)\n\n# Fit with different methods\ngmm_em = fit(EM(3), data)\ngmm_pca = fit(PCAEM(3, 1), data)\ngmm_factor = fit(FactorEM(3, 1), data)\n\n# Evaluate\nprintln(\"EM log-likelihood: \", mean(logpdf(gmm_em, data)))\nprintln(\"PCAEM log-likelihood: \", mean(logpdf(gmm_pca, data)))\nprintln(\"FactorEM log-likelihood: \", mean(logpdf(gmm_factor, data)))","category":"page"},{"location":"fitting/#Weighted-Fitting","page":"Overview","title":"Weighted Fitting","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"# Create weights based on data values\nweights = [data[1, i] > 0 ? 1.0 : 0.5 for i in 1:size(data, 2)]\n\n# Fit with weights (only FactorEM supports this)\ngmm_weighted = fit(FactorEM(3, 1), data, weights)","category":"page"},{"location":"fitting/#High-Dimensional-Data","page":"Overview","title":"High-Dimensional Data","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"# For high-dimensional data\nhigh_dim_data = randn(100, 50)\n\n# Use low-rank methods\ngmm_pca = fit(PCAEM(3, 10), high_dim_data)\ngmm_factor = fit(FactorEM(3, 10), high_dim_data)","category":"page"},{"location":"fitting/#Related-Documentation","page":"Overview","title":"Related Documentation","text":"","category":"section"},{"location":"fitting/","page":"Overview","title":"Overview","text":"LRDMvNormal: Learn about the low-rank plus diagonal distribution used by PCAEM and FactorEM methods ","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page provides simple working examples demonstrating typical workflows for fitting, prediction, and weighted fitting.","category":"page"},{"location":"examples/#Example-1:-Basic-Fitting","page":"Examples","title":"Example 1: Basic Fitting","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to fit GMMs using the three different methods.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using StructuredGaussianMixtures\n\n# Generate some data\ndata = randn(2, 1000)\n\n# Fit with different methods\ngmm_em = fit(EM(3), data)\ngmm_pca = fit(PCAEM(3, 1), data)\ngmm_factor = fit(FactorEM(3, 1), data)\n\n# Evaluate performance\nprintln(\"EM log-likelihood: \", mean(logpdf(gmm_em, data)))\nprintln(\"PCAEM log-likelihood: \", mean(logpdf(gmm_pca, data)))\nprintln(\"FactorEM log-likelihood: \", mean(logpdf(gmm_factor, data)))","category":"page"},{"location":"examples/#Example-2:-High-Dimensional-Data","page":"Examples","title":"Example 2: High-Dimensional Data","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example demonstrates the effectiveness of low-rank methods for high-dimensional data.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# High-dimensional data\nn_features = 100\nn_samples = 50\ndata = randn(n_features, n_samples)\n\n# Compare methods\nprintln(\"Fitting EM model...\")\n@time gmm_em = fit(EM(3), data)\n\nprintln(\"Fitting PCAEM model...\")\n@time gmm_pca = fit(PCAEM(3, 10), data)\n\nprintln(\"Fitting FactorEM model...\")\n@time gmm_factor = fit(FactorEM(3, 10), data)\n\n# Compare performance\nprintln(\"EM log-likelihood: \", mean(logpdf(gmm_em, data)))\nprintln(\"PCAEM log-likelihood: \", mean(logpdf(gmm_pca, data)))\nprintln(\"FactorEM log-likelihood: \", mean(logpdf(gmm_factor, data)))","category":"page"},{"location":"examples/#Example-3:-Conditional-Prediction","page":"Examples","title":"Example 3: Conditional Prediction","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example demonstrates how to perform conditional prediction.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Make prediction\nx_query = [0.5]\nposterior = predict(gmm, x_query)\n\n# Generate samples from posterior\nsamples = rand(posterior, 100)\nprintln(\"Posterior mean: \", mean(samples))\nprintln(\"Posterior variance: \", var(samples))","category":"page"},{"location":"examples/#Example-4:-Weighted-Fitting","page":"Examples","title":"Example 4: Weighted Fitting","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to fit a GMM with weighted data points.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Generate data\ndata = randn(2, 1000)\n\n# Create weights based on data values\nweights = [data[1, i] > 0 ? 1.0 : 0.5 for i in 1:size(data, 2)]\n\n# Fit with weights (only FactorEM supports this)\ngmm_weighted = fit(FactorEM(3, 1), data, weights)\n\n# Print results\nprintln(\"Number of samples with weight 1: \", sum(weights .== 1.0))\nprintln(\"Number of samples with weight 0.5: \", sum(weights .== 0.5))\nprintln(\"Weighted log-likelihood: \", mean(logpdf(gmm_weighted, data)))","category":"page"},{"location":"examples/#Example-5:-LRDMvNormal-Usage","page":"Examples","title":"Example 5: LRDMvNormal Usage","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to work directly with LRDMvNormal distributions.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using StructuredGaussianMixtures\n\n# Create a low-rank distribution\nn_features = 10\nrank = 3\nμ = randn(n_features)\nF = randn(n_features, rank)\nD = ones(n_features) * 0.1\n\ndist = LRDMvNormal(μ, F, D)\n\n# Basic properties\nprintln(\"Dimension: \", length(dist))\nprintln(\"Rank: \", rank(dist))\nprintln(\"Mean: \", mean(dist)[1:5])  # First 5 elements\n\n# Generate samples\nsamples = rand(dist, 1000)\nprintln(\"Sample shape: \", size(samples))\n\n# Compute log probability\nlog_prob = logpdf(dist, samples[:, 1])\nprintln(\"Log probability: \", log_prob)","category":"page"},{"location":"examples/#Example-6:-Partial-Prediction","page":"Examples","title":"Example 6: Partial Prediction","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to predict specific dimensions.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Fit a 5D GMM\ndata = randn(5, 1000)\ngmm = fit(EM(3), data)\n\n# Observe dimensions 1 and 3, predict dimensions 2, 4, and 5\nobserved_values = [0.5, -0.2]\nobserved_dims = [1, 3]\noutput_dims = [2, 4, 5]\n\nposterior = predict(gmm, observed_values; \n    input_indices=observed_dims, \n    output_indices=output_dims)\n\n# Generate samples\nsamples = rand(posterior, 100)\nprintln(\"Predicted dimensions shape: \", size(samples))","category":"page"},{"location":"examples/#Example-7:-Multiple-Query-Points","page":"Examples","title":"Example 7: Multiple Query Points","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to make predictions for multiple query points.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Predict for multiple query points\nquery_points = [[0.5], [-0.2], [1.1]]\nposteriors = [predict(gmm, q) for q in query_points]\n\n# Generate samples for each\nsamples = [rand(p, 50) for p in posteriors]\n\n# Print results\nfor (i, (q, s)) in enumerate(zip(query_points, samples))\n    println(\"Query $i: mean = \", mean(s), \", variance = \", var(s))\nend","category":"page"},{"location":"examples/#Running-the-Examples","page":"Examples","title":"Running the Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"To run these examples:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Install dependencies:\nusing Pkg\nPkg.add([\"StructuredGaussianMixtures\", \"Distributions\", \"LinearAlgebra\", \"Random\", \"Statistics\"])\nLoad the package:\nusing StructuredGaussianMixtures\nRun examples: Copy and paste any example into a Julia session.","category":"page"},{"location":"prediction/#Prediction","page":"Prediction","title":"Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"This page describes how to perform conditional prediction with fitted Gaussian Mixture Models using StructuredGaussianMixtures.jl.","category":"page"},{"location":"prediction/#Overview","page":"Prediction","title":"Overview","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"The predict function computes posterior distributions over unobserved dimensions given observed values. This is useful for:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"Missing data imputation: Fill in missing values based on observed data\nConditional sampling: Generate samples from the posterior distribution\nUncertainty quantification: Assess uncertainty in predictions","category":"page"},{"location":"prediction/#Function-Signature","page":"Prediction","title":"Function Signature","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"predict(gmm, x; input_indices=1:length(x), output_indices=length(x)+1:length(gmm))","category":"page"},{"location":"prediction/#Parameters","page":"Prediction","title":"Parameters","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"gmm: A fitted MixtureModel\nx: Observed values\ninput_indices: Indices of observed dimensions (default: 1:length(x))\noutput_indices: Indices of dimensions to predict (default: length(x)+1:length(gmm))","category":"page"},{"location":"prediction/#Basic-Usage","page":"Prediction","title":"Basic Usage","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"using StructuredGaussianMixtures\n\n# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Make predictions\nquery_point = [0.5]  # Observed value for first dimension\nposterior = predict(gmm, query_point)  # Posterior over second dimension","category":"page"},{"location":"prediction/#Predict-Functions","page":"Prediction","title":"Predict Functions","text":"","category":"section"},{"location":"prediction/#MvNormal-Prediction","page":"Prediction","title":"MvNormal Prediction","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{MvNormal, AbstractVector, Union{AbstractRange, Vector{Int64}}, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::MvNormal, x::AbstractVector, input_indices::Union{Vector{Int},AbstractRange}, output_indices::Union{Vector{Int},AbstractRange})\n\nCompute the conditional distribution of the output indices given the input indices using the Schur complement. Returns a new MvNormal distribution representing the conditional distribution.\n\nArguments\n\ndist: The multivariate normal distribution\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables\noutput_indices: The indices of the variables to predict\n\nReturns\n\nA new MvNormal distribution representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#LRDMvNormal-Prediction","page":"Prediction","title":"LRDMvNormal Prediction","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{LRDMvNormal, AbstractVector, Union{AbstractRange, Vector{Int64}}, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::LRDMvNormal, x::AbstractVector, input_indices::Union{Vector{Int},AbstractRange}, output_indices::Union{Vector{Int},AbstractRange})\n\nCompute the conditional distribution of the output indices given the input indices using the Schur complement. Returns a new LRDMvNormal distribution representing the conditional distribution. This implementation is efficient for low-rank plus diagonal covariance structure.\n\nArguments\n\ndist: The low-rank plus diagonal multivariate normal distribution\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables\noutput_indices: The indices of the variables to predict\n\nReturns\n\nA new LRDMvNormal distribution representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#MixtureModel-Prediction","page":"Prediction","title":"MixtureModel Prediction","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{MultivariateMixture, AbstractVector, Union{AbstractRange, Vector{Int64}}, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::MultivariateMixture, x::AbstractVector, input_indices::Union{Vector{Int},AbstractRange}, output_indices::Union{Vector{Int},AbstractRange})\n\nCompute the conditional distribution of the output indices given the input indices for a mixture model. Returns a new mixture model where each component is the conditional distribution of the corresponding component, and the weights are updated based on the log density of x under the marginal distributions.\n\nArguments\n\ndist: The multivariate mixture distribution\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables\noutput_indices: The indices of the variables to predict\n\nReturns\n\nA new mixture model representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#Convenience-Function","page":"Prediction","title":"Convenience Function","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{Union{LRDMvNormal, MvNormal, MultivariateMixture}, AbstractVector}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::Union{MvNormal,LRDMvNormal,MultivariateMixture}, x::AbstractVector; \n       input_indices::Union{Vector{Int},AbstractRange} = 1:length(x), \n       output_indices::Union{Vector{Int},AbstractRange} = length(x)+1:length(mean(dist)))\n\nCompute the conditional distribution of the output indices given the input indices using the Schur complement. Returns a new distribution representing the conditional distribution.\n\nArguments\n\ndist: The multivariate normal distribution (MvNormal or LRDMvNormal)\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables (default: first length(x) indices)\noutput_indices: The indices of the variables to predict (default: remaining indices)\n\nReturns\n\nA new distribution representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#Marginal-Functions","page":"Prediction","title":"Marginal Functions","text":"","category":"section"},{"location":"prediction/#Marginal-Distribution","page":"Prediction","title":"Marginal Distribution","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.marginal-Tuple{MvNormal, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.marginal","text":"marginal(dist::MvNormal, indices::Union{Vector{Int},AbstractRange})\n\nCompute the marginal distribution over the specified indices. Returns a new MvNormal distribution representing the marginal.\n\nArguments\n\ndist: The multivariate normal distribution\nindices: The indices to marginalize over\n\nReturns\n\nA new MvNormal distribution representing the marginal\n\n\n\n\n\n","category":"method"},{"location":"prediction/#StructuredGaussianMixtures.marginal-Tuple{LRDMvNormal, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.marginal","text":"marginal(dist::LRDMvNormal, indices::Union{Vector{Int},AbstractRange})\n\nCompute the marginal distribution over the specified indices. Returns a new LRDMvNormal distribution representing the marginal.\n\nArguments\n\ndist: The low-rank plus diagonal multivariate normal distribution\nindices: The indices to marginalize over\n\nReturns\n\nA new LRDMvNormal distribution representing the marginal\n\n\n\n\n\n","category":"method"},{"location":"prediction/#Simple-Examples","page":"Prediction","title":"Simple Examples","text":"","category":"section"},{"location":"prediction/#2D-GMM-Prediction","page":"Prediction","title":"2D GMM Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Make prediction\nx_query = [0.5]\nposterior = predict(gmm, x_query)\n\n# Generate samples from posterior\nsamples = rand(posterior, 100)\nprintln(\"Posterior mean: \", mean(samples))","category":"page"},{"location":"prediction/#High-Dimensional-Prediction","page":"Prediction","title":"High-Dimensional Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Fit a high-dimensional GMM\ndata = randn(10, 1000)\ngmm = fit(PCAEM(3, 3), data)\n\n# Predict multiple dimensions\nobserved_dims = [1, 3, 5]\nquery_values = [0.5, -0.2, 1.1]\nunobserved_dims = [2, 4, 6, 7, 8, 9, 10]\n\nposterior = predict(gmm, query_values; \n    input_indices=observed_dims, \n    output_indices=unobserved_dims)\n\n# Generate samples\nsamples = rand(posterior, 100)\nprintln(\"Predicted dimensions shape: \", size(samples))","category":"page"},{"location":"prediction/#Partial-Prediction","page":"Prediction","title":"Partial Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Fit a 5D GMM\ndata = randn(5, 1000)\ngmm = fit(EM(3), data)\n\n# Observe dimensions 1 and 3, predict dimensions 2, 4, and 5\nobserved_values = [0.5, -0.2]\nobserved_dims = [1, 3]\noutput_dims = [2, 4, 5]\n\nposterior = predict(gmm, observed_values; \n    input_indices=observed_dims, \n    output_indices=output_dims)\n\nsamples = rand(posterior, 100)\nprintln(\"Predicted dimensions shape: \", size(samples))","category":"page"},{"location":"prediction/#Mathematical-Background","page":"Prediction","title":"Mathematical Background","text":"","category":"section"},{"location":"prediction/#Conditional-Gaussian-Distribution","page":"Prediction","title":"Conditional Gaussian Distribution","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"For a Gaussian Mixture Model with components k = 1 ldots K, the posterior distribution given observed values x_obs is:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"p(x_unobs mid x_obs) = sum_k=1^K w_k mathcalN(x_unobs mid mu_k^unobs Sigma_k^unobs)","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"where:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"w_k\nare the posterior component weights\nmu_k^unobs\nis the conditional mean for component k\nSigma_k^unobs\nis the conditional covariance for component k","category":"page"},{"location":"prediction/#Component-Weight-Updates","page":"Prediction","title":"Component Weight Updates","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"The posterior component weights are computed as:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"w_k = fracpi_k mathcalN(x_obs mid mu_k^obs Sigma_k^obs)sum_j=1^K pi_j mathcalN(x_obs mid mu_j^obs Sigma_j^obs)","category":"page"},{"location":"prediction/#Conditional-Parameters","page":"Prediction","title":"Conditional Parameters","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"For each component k, the conditional parameters are:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"mu_k^unobs = mu_k^unobs + Sigma_k^unobsobs (Sigma_k^obs)^-1 (x_obs - mu_k^obs)","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"Sigma_k^unobs = Sigma_k^unobsunobs - Sigma_k^unobsobs (Sigma_k^obs)^-1 Sigma_k^obsunobs","category":"page"},{"location":"prediction/#Advanced-Usage","page":"Prediction","title":"Advanced Usage","text":"","category":"section"},{"location":"prediction/#Multiple-Query-Points","page":"Prediction","title":"Multiple Query Points","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Predict for multiple query points\nquery_points = [[0.5], [-0.2], [1.1]]\nposteriors = [predict(gmm, q) for q in query_points]\n\n# Generate samples for each\nsamples = [rand(p, 50) for p in posteriors]","category":"page"},{"location":"prediction/#Uncertainty-Quantification","page":"Prediction","title":"Uncertainty Quantification","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Compute posterior statistics\nposterior = predict(gmm, query_point)\nsamples = rand(posterior, 1000)\n\n# Mean and variance\nposterior_mean = mean(samples, dims=2)\nposterior_var = var(samples, dims=2)\n\n# Confidence intervals\nposterior_quantiles = quantile(samples, [0.025, 0.975], dims=2)","category":"page"},{"location":"prediction/#Performance-Considerations","page":"Prediction","title":"Performance Considerations","text":"","category":"section"},{"location":"prediction/#Computational-Complexity","page":"Prediction","title":"Computational Complexity","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"EM: O(m³) for covariance operations\nPCAEM: O(r³) where r is the PCA rank\nFactorEM: O(r³) where r is the factor rank","category":"page"},{"location":"prediction/#Memory-Usage","page":"Prediction","title":"Memory Usage","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"For high-dimensional data, consider using low-rank methods:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# For 1000-dimensional data\nn_features = 1000\ndata = randn(n_features, 100)\n\n# Use low-rank methods for efficiency\ngmm = fit(PCAEM(3, 10), data)  # 10-dimensional PCA\n# or\ngmm = fit(FactorEM(3, 10), data)  # Rank-10 factor analysis","category":"page"},{"location":"#StructuredGaussianMixtures.jl","page":"Home","title":"StructuredGaussianMixtures.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for fitting and conditional prediction of Gaussian Mixture Models (GMMs) with structured covariance matrices.","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements three main fitting methods for Gaussian Mixture Models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"EM: Standard Expectation Maximization for full covariance matrices\nPCAEM: Mixture of Probabilistic Principal Component Analysis models\nFactorEM: Mixture of Factor Analyzers with low-rank plus diagonal covariance structure","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package also supports conditional prediction, allowing you to compute posterior distributions over unobserved dimensions given observed values.","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using StructuredGaussianMixtures\n\n# Fit a GMM using EM\ndata = randn(2, 1000)  # 2D data with 1000 samples\ngmm = fit(EM(3), data)  # 3-component GMM\n\n# Make predictions\nquery_point = [0.5]\nposterior = predict(gmm, query_point)  # Posterior over second dimension","category":"page"},{"location":"#Documentation-Sections","page":"Home","title":"Documentation Sections","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Fitting Methods: Learn about the different fitting algorithms and when to use each\nPrediction: Understand conditional prediction and posterior computation\nExamples: Complete working examples from the test files","category":"page"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Efficient high-dimensional fitting: Low-rank covariance structures for m ≫ n settings\nConditional prediction: Compute posterior distributions over unobserved variables\nWeighted fitting: Support for weighted data points in model fitting\nMultiple initialization methods: K-means, random, and custom initialization\nComprehensive evaluation: Log-likelihood, JSD, and visualization tools","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StructuredGaussianMixtures\")","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The main functions and types are documented in their respective sections:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fitting Methods: EM, PCAEM, FactorEM, and fit functions\nLRDMvNormal: Low-rank plus diagonal distribution\nPrediction: predict functions for conditional inference","category":"page"},{"location":"lrdmvnormal/#LRDMvNormal","page":"LRDMvNormal","title":"LRDMvNormal","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"This page documents the LRDMvNormal class, which represents low-rank plus diagonal multivariate normal distributions.","category":"page"},{"location":"lrdmvnormal/#Overview","page":"LRDMvNormal","title":"Overview","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"The LRDMvNormal distribution represents a multivariate normal distribution with covariance matrix of the form:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"Sigma = FF + D","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"where:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"F\nis a low-rank factor matrix of size m times r where r ll m\nD\nis a diagonal matrix\nThe full covariance matrix is never explicitly formed for efficiency","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"This structure is particularly useful for high-dimensional data where m gg n (number of features much larger than number of samples).","category":"page"},{"location":"lrdmvnormal/#Constructor","page":"LRDMvNormal","title":"Constructor","text":"","category":"section"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.LRDMvNormal","page":"LRDMvNormal","title":"StructuredGaussianMixtures.LRDMvNormal","text":"LRDMvNormal\n\nLow-rank plus diagonal multivariate normal distribution. The covariance matrix is represented as Σ = FF' + D, where F is a low-rank factor matrix and D is a diagonal matrix.\n\nFields\n\nμ: Mean vector\nF: Low-rank factor matrix\nD: Diagonal vector\nrank: Rank of the low-rank component\n\nNotes\n\nThe covariance matrix is never explicitly formed\nAll operations use the low-rank plus diagonal structure for efficiency\n\n\n\n\n\n","category":"type"},{"location":"lrdmvnormal/#Usage","page":"LRDMvNormal","title":"Usage","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"using StructuredGaussianMixtures\n\n# Create a 10-dimensional distribution with rank-3 structure\nμ = randn(10)           # Mean vector\nF = randn(10, 3)        # Low-rank factor (10×3)\nD = ones(10)            # Diagonal vector\ndist = LRDMvNormal(μ, F, D)","category":"page"},{"location":"lrdmvnormal/#Distribution-Interface","page":"LRDMvNormal","title":"Distribution Interface","text":"","category":"section"},{"location":"lrdmvnormal/#Basic-Properties","page":"LRDMvNormal","title":"Basic Properties","text":"","category":"section"},{"location":"lrdmvnormal/#Base.length-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"Base.length","text":"length(d::LRDMvNormal)\n\nReturn the dimension of the distribution.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Base.size-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"Base.size","text":"size(d::LRDMvNormal)\n\nReturn the size of the distribution as a tuple (dimension,).\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Statistics.mean-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"Statistics.mean","text":"mean(d::LRDMvNormal)\n\nReturn the mean vector of the distribution.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Statistics.cov-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"Statistics.cov","text":"cov(d::LRDMvNormal)\n\nReturn the full covariance matrix FF' + D.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Low-Rank-Structure-Access","page":"LRDMvNormal","title":"Low-Rank Structure Access","text":"","category":"section"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.rank-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"StructuredGaussianMixtures.rank","text":"rank(d::LRDMvNormal)\n\nReturn the rank of the low-rank component.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.low_rank_factor-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"StructuredGaussianMixtures.low_rank_factor","text":"low_rank_factor(d::LRDMvNormal)\n\nReturn the low-rank factor matrix F.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.diagonal-Tuple{LRDMvNormal}","page":"LRDMvNormal","title":"StructuredGaussianMixtures.diagonal","text":"diagonal(d::LRDMvNormal)\n\nReturn the diagonal vector D.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Probability-and-Sampling","page":"LRDMvNormal","title":"Probability and Sampling","text":"","category":"section"},{"location":"lrdmvnormal/#Log-Probability-Density","page":"LRDMvNormal","title":"Log Probability Density","text":"","category":"section"},{"location":"lrdmvnormal/#Distributions.logpdf-Tuple{LRDMvNormal, AbstractVector}","page":"LRDMvNormal","title":"Distributions.logpdf","text":"logpdf(d::LRDMvNormal, x::AbstractVector)\n\nCompute the log probability density function at x. Uses the matrix inversion lemma for efficient computation.\n\nArguments\n\nd: The LRDMvNormal distribution\nx: The point at which to evaluate the log PDF\n\nReturns\n\nThe log probability density at x\n\nNotes\n\nUses the matrix inversion lemma: (FF' + D)^(-1) = D^(-1) - D^(-1)F(I + F'D^(-1)F)^(-1)F'*D^(-1)\nComputes the determinant efficiently: det(FF' + D) = det(D) * det(I + F'D^(-1)*F)\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Random-Sampling","page":"LRDMvNormal","title":"Random Sampling","text":"","category":"section"},{"location":"lrdmvnormal/#Distributions._rand!-Tuple{AbstractRNG, LRDMvNormal, VecOrMat}","page":"LRDMvNormal","title":"Distributions._rand!","text":"_rand!(rng::AbstractRNG, d::LRDMvNormal, x::VecOrMat)\n\nGenerate random samples in-place from the distribution.\n\nArguments\n\nrng: Random number generator\nd: The LRDMvNormal distribution\nx: Vector or matrix to fill with random samples\n\nReturns\n\nThe filled vector/matrix x\n\nNotes\n\nUses the decomposition: X = μ + FZ₁ + sqrt(D)Z₂ where Z₁, Z₂ are standard normal\nFor matrices, each column is a sample\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Distributions._rand!-Tuple{AbstractRNG, LRDMvNormal, AbstractVector}","page":"LRDMvNormal","title":"Distributions._rand!","text":"_rand!(rng::AbstractRNG, d::LRDMvNormal, x::AbstractVector)\n\nGenerate a random sample in-place from the distribution.\n\nArguments\n\nrng: Random number generator\nd: The LRDMvNormal distribution\nx: Vector to fill with random sample\n\nReturns\n\nThe filled vector x\n\nNotes\n\nUses the decomposition: X = μ + FZ₁ + sqrt(D)Z₂ where Z₁, Z₂ are standard normal\nHandles AbstractVector types that don't support randn!\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Simple-Examples","page":"LRDMvNormal","title":"Simple Examples","text":"","category":"section"},{"location":"lrdmvnormal/#Creating-and-Using-LRDMvNormal","page":"LRDMvNormal","title":"Creating and Using LRDMvNormal","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"using StructuredGaussianMixtures\n\n# Create a low-rank distribution\nn_features = 100\nrank = 5\nμ = randn(n_features)\nF = randn(n_features, rank)\nD = ones(n_features) * 0.1\n\ndist = LRDMvNormal(μ, F, D)\n\n# Basic properties\nprintln(\"Dimension: \", length(dist))\nprintln(\"Rank: \", rank(dist))\nprintln(\"Mean: \", mean(dist)[1:5])  # First 5 elements\n\n# Generate samples\nsamples = rand(dist, 1000)\nprintln(\"Sample shape: \", size(samples))\n\n# Compute log probability\nlog_prob = logpdf(dist, samples[:, 1])\nprintln(\"Log probability: \", log_prob)","category":"page"},{"location":"lrdmvnormal/#Efficient-Operations","page":"LRDMvNormal","title":"Efficient Operations","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"# The covariance matrix is never explicitly formed\n# This is efficient even for high dimensions\nn_features = 1000\nrank = 10\n\nμ = randn(n_features)\nF = randn(n_features, rank)\nD = ones(n_features) * 0.1\n\ndist = LRDMvNormal(μ, F, D)\n\n# This is efficient - no O(n³) operations\nlog_prob = logpdf(dist, randn(n_features))\n\n# Sampling is also efficient\nsamples = rand(dist, 100)","category":"page"},{"location":"lrdmvnormal/#Accessing-Components","page":"LRDMvNormal","title":"Accessing Components","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"# Get the low-rank structure\nF_matrix = low_rank_factor(dist)\nD_vector = diagonal(dist)\nrank_val = rank(dist)\n\nprintln(\"F shape: \", size(F_matrix))\nprintln(\"D length: \", length(D_vector))\nprintln(\"Rank: \", rank_val)\n\n# Full covariance (only for small dimensions!)\nfull_cov = cov(dist)\nprintln(\"Full covariance shape: \", size(full_cov))","category":"page"},{"location":"lrdmvnormal/#Mathematical-Background","page":"LRDMvNormal","title":"Mathematical Background","text":"","category":"section"},{"location":"lrdmvnormal/#Efficient-Log-Likelihood-Computation","page":"LRDMvNormal","title":"Efficient Log-Likelihood Computation","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"The log probability density is computed efficiently using the matrix inversion lemma:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"(FF + D)^-1 = D^-1 - D^-1F(I + FD^-1F)^-1FD^-1","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"This avoids forming the full covariance matrix and reduces computational complexity from O(m³) to O(r³) where r ≪ m.","category":"page"},{"location":"lrdmvnormal/#Determinant-Computation","page":"LRDMvNormal","title":"Determinant Computation","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"The determinant is computed efficiently as:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"det(FF + D) = det(D) cdot det(I + FD^-1F)","category":"page"},{"location":"lrdmvnormal/#Sampling","page":"LRDMvNormal","title":"Sampling","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"Samples are generated using the decomposition:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"X = mu + FZ_1 + sqrtDZ_2","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"where Z_1 and Z_2 are independent standard normal random vectors.","category":"page"},{"location":"lrdmvnormal/#Performance-Characteristics","page":"LRDMvNormal","title":"Performance Characteristics","text":"","category":"section"},{"location":"lrdmvnormal/#Computational-Complexity","page":"LRDMvNormal","title":"Computational Complexity","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"Operation Complexity Notes\nLog-likelihood O(r³) Uses matrix inversion lemma\nSampling O(mr) Efficient decomposition\nMemory O(mr) Stores F and D, not full covariance","category":"page"},{"location":"lrdmvnormal/#Memory-Usage","page":"LRDMvNormal","title":"Memory Usage","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"For a distribution with m features and rank r:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"Storage: O(mr + m) = O(mr) for F and D\nTraditional: O(m²) for full covariance matrix\nSavings: O(m²mr) = O(mr) for typical r ll m","category":"page"},{"location":"lrdmvnormal/#When-to-Use","page":"LRDMvNormal","title":"When to Use","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"High-dimensional data where m gg n\nLow-rank structure in the data\nMemory constraints preventing full covariance storage\nEfficient sampling requirements","category":"page"},{"location":"lrdmvnormal/#Integration-with-GMMs","page":"LRDMvNormal","title":"Integration with GMMs","text":"","category":"section"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"The LRDMvNormal distribution is used internally by PCAEM and FactorEM methods:","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"# PCAEM creates LRDMvNormal components\ngmm = fit(PCAEM(3, 5), data)\nfor comp in components(gmm)\n    println(\"Component rank: \", rank(comp))\nend\n\n# FactorEM also creates LRDMvNormal components\ngmm = fit(FactorEM(3, 5), data)\nfor comp in components(gmm)\n    println(\"Component rank: \", rank(comp))\nend","category":"page"},{"location":"lrdmvnormal/","page":"LRDMvNormal","title":"LRDMvNormal","text":"","category":"page"}]
}
