var documenterSearchIndex = {"docs":
[{"location":"fitting/#Fitting-Methods","page":"Fitting","title":"Fitting Methods","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"This page describes the three main fitting methods implemented in StructuredGaussianMixtures.jl: EM, PCAEM, and FactorEM.","category":"page"},{"location":"fitting/#Overview","page":"Fitting","title":"Overview","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"All fitting methods implement the GMMFitMethod interface and can be used with the fit function:","category":"page"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"gmm = fit(fitmethod, data)","category":"page"},{"location":"fitting/#GMMFitMethod-Interface","page":"Fitting","title":"GMMFitMethod Interface","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.GMMFitMethod","page":"Fitting","title":"StructuredGaussianMixtures.GMMFitMethod","text":"GMMFitMethod\n\nAbstract type for Gaussian Mixture Model fitting methods.\n\n\n\n\n\n","category":"type"},{"location":"fitting/#EM:-Standard-Expectation-Maximization","page":"Fitting","title":"EM: Standard Expectation Maximization","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"The EM method fits Gaussian Mixture Models with full covariance matrices using standard Expectation Maximization.","category":"page"},{"location":"fitting/#Constructor","page":"Fitting","title":"Constructor","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.EM","page":"Fitting","title":"StructuredGaussianMixtures.EM","text":"EM\n\nStandard Expectation Maximization for fitting Gaussian Mixture Models.\n\nFields\n\nn_components: Number of mixture components\nmethod: Initialization method (:kmeans, :rand, etc.)\nkind: Covariance structure (:full, :diag, etc.)\nnInit: Number of initializations\nnIter: Maximum number of iterations\nnFinal: Number of final iterations\n\n\n\n\n\n","category":"type"},{"location":"fitting/#Usage","page":"Fitting","title":"Usage","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"using StructuredGaussianMixtures\n\n# Basic usage\nfitmethod = EM(3)\ngmm = fit(fitmethod, data)\n\n# With custom parameters\nfitmethod = EM(5; method=:rand, kind=:full, nInit=100, nIter=20)\ngmm = fit(fitmethod, data)","category":"page"},{"location":"fitting/#Fit-Method","page":"Fitting","title":"Fit Method","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{EM, Matrix}","page":"Fitting","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::EM, x::Matrix)\n\nFit a Gaussian Mixture Model using Expectation Maximization.\n\nArguments\n\nfitmethod: The EM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\n\nReturns\n\nA MixtureModel of MvNormal distributions\n\nNotes\n\nUses GaussianMixtures.jl's GMM implementation\nSupports different initialization methods and covariance structures\n\n\n\n\n\n","category":"method"},{"location":"fitting/#PCAEM:-Mixture-of-Probabilistic-Principal-Component-Analysis","page":"Fitting","title":"PCAEM: Mixture of Probabilistic Principal Component Analysis","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"PCAEM fits a GMM in PCA-reduced space and transforms back to the original space, effectively learning low-rank covariance structures.","category":"page"},{"location":"fitting/#Constructor-2","page":"Fitting","title":"Constructor","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.PCAEM","page":"Fitting","title":"StructuredGaussianMixtures.PCAEM","text":"PCAEM\n\nFits a structured GMM by fitting a GMM in PCA-compressed space.\n\nFields\n\nn_components: Number of mixture components\nrank: Number of principal components to use\ngmm_method: Initialization method for GMM (:kmeans, :rand, etc.)\ngmm_kind: Covariance structure for GMM (:full, :diag, etc.)\ngmm_nInit: Number of GMM initializations\ngmm_nIter: Maximum number of GMM iterations\ngmm_nFinal: Number of final GMM iterations\n\n\n\n\n\n","category":"type"},{"location":"fitting/#Usage-2","page":"Fitting","title":"Usage","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"# Basic usage\nfitmethod = PCAEM(3, 2)\ngmm = fit(fitmethod, data)\n\n# With custom parameters\nfitmethod = PCAEM(5, 3; gmm_method=:rand, gmm_nInit=100)\ngmm = fit(fitmethod, data)","category":"page"},{"location":"fitting/#Fit-Method-2","page":"Fitting","title":"Fit Method","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{PCAEM, Matrix}","page":"Fitting","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::PCAEM, x::Matrix)\n\nThis method first performs PCA to reduce dimensionality, then fits a GMM in the reduced space, and finally transforms the components back to the original space as LRDMvNormal distributions.\n\nArguments\n\nfitmethod: The PCAEM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\n\nReturns\n\nA MixtureModel of LRDMvNormal distributions\n\nNotes\n\nUses PCA for dimensionality reduction\nFits GMM in the reduced space\nTransforms components back to original space with low-rank plus diagonal structure\nThe diagonal noise term is estimated from PCA reconstruction error\n\n\n\n\n\n","category":"method"},{"location":"fitting/#FactorEM:-Mixture-of-Factor-Analyzers","page":"Fitting","title":"FactorEM: Mixture of Factor Analyzers","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"FactorEM directly fits GMMs with covariance matrices constrained to the form Σ = FF' + D, where F is a low-rank factor matrix and D is diagonal.","category":"page"},{"location":"fitting/#Constructor-3","page":"Fitting","title":"Constructor","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.FactorEM","page":"Fitting","title":"StructuredGaussianMixtures.FactorEM","text":"FactorEM\n\nMixture of Factor Analyzers model. Directly fits a mixture of low-rank plus diagonal Gaussian distributions.\n\nFields\n\nn_components: Number of mixture components\nrank: Rank of the low-rank factor\ngmm_method: Initialization method for GMM (:kmeans, :rand, etc.)\ngmm_nInit: Number of GMM initializations\ngmm_nIter: Maximum number of GMM iterations\ngmm_nFinal: Number of final GMM iterations\n\n\n\n\n\n","category":"type"},{"location":"fitting/#Usage-3","page":"Fitting","title":"Usage","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"# Basic usage\nfitmethod = FactorEM(3, 2)\ngmm = fit(fitmethod, data)\n\n# With custom parameters\nfitmethod = FactorEM(5, 3; initialization_method=:rand, nInit=10, nIter=20)\ngmm = fit(fitmethod, data)\n\n# With weighted data\nweights = ones(size(data, 2))  # Equal weights\ngmm = fit(fitmethod, data, weights)","category":"page"},{"location":"fitting/#Fit-Methods","page":"Fitting","title":"Fit Methods","text":"","category":"section"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{FactorEM, Matrix}","page":"Fitting","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::FactorEM, x::Matrix)\n\nFit a Mixture of Factor Analyzers model using Expectation Maximization. This method directly fits a mixture of low-rank plus diagonal Gaussian distributions.\n\nArguments\n\nfitmethod: The FactorEM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\n\nReturns\n\nA MixtureModel of LRDMvNormal distributions\n\nNotes\n\nDirectly fits the low-rank plus diagonal structure\nMore computationally intensive than PCAEM but potentially more accurate\nNot yet implemented\n\n\n\n\n\n","category":"method"},{"location":"fitting/#StructuredGaussianMixtures.fit-Tuple{FactorEM, Matrix, Vector}","page":"Fitting","title":"StructuredGaussianMixtures.fit","text":"fit(fitmethod::FactorEM, x::Matrix, weights::Vector)\n\nFit a Mixture of Factor Analyzers model using Expectation Maximization with weighted data points.\n\nArguments\n\nfitmethod: The FactorEM fitting method configuration\nx: The data matrix (nsamples × nfeatures)\nweights: Vector of weights for each data point\n\nReturns\n\nA MixtureModel of LRDMvNormal distributions\n\nNotes\n\nSupports weighted data points for importance sampling or missing data scenarios\nWeights are automatically normalized to sum to 1\nUses the same EM algorithm but with weighted responsibilities\n\n\n\n\n\n","category":"method"},{"location":"fitting/#Method-Comparison","page":"Fitting","title":"Method Comparison","text":"","category":"section"},{"location":"fitting/#When-to-Use-Each-Method","page":"Fitting","title":"When to Use Each Method","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"Method Best For Covariance Structure Computational Cost Weighted Fitting\nFactorEM High-dimensional data, direct low-rank fitting Low-rank + diagonal O(r³ + mr² + nmr) where r < m ✅\nEM Low-dimensional data, full covariance needed Full O(m³ + nm²) ❌\nPCAEM High-dimensional data with shared low-dimensional structure Low-rank + diagonal O(r³ + mr² + nr² + min(n²m,nm²)) where r < m ❌","category":"page"},{"location":"fitting/#Simple-Examples","page":"Fitting","title":"Simple Examples","text":"","category":"section"},{"location":"fitting/#Basic-Fitting","page":"Fitting","title":"Basic Fitting","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"using StructuredGaussianMixtures\n\n# Generate some data\ndata = randn(2, 1000)\n\n# Fit with different methods\ngmm_em = fit(EM(3), data)\ngmm_pca = fit(PCAEM(3, 1), data)\ngmm_factor = fit(FactorEM(3, 1), data)\n\n# Evaluate\nprintln(\"EM log-likelihood: \", mean(logpdf(gmm_em, data)))\nprintln(\"PCAEM log-likelihood: \", mean(logpdf(gmm_pca, data)))\nprintln(\"FactorEM log-likelihood: \", mean(logpdf(gmm_factor, data)))","category":"page"},{"location":"fitting/#Weighted-Fitting","page":"Fitting","title":"Weighted Fitting","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"# Create weights based on data values\nweights = [data[1, i] > 0 ? 1.0 : 0.5 for i in 1:size(data, 2)]\n\n# Fit with weights (only FactorEM supports this)\ngmm_weighted = fit(FactorEM(3, 1), data, weights)","category":"page"},{"location":"fitting/#High-Dimensional-Data","page":"Fitting","title":"High-Dimensional Data","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"# For high-dimensional data\nhigh_dim_data = randn(100, 50)\n\n# Use low-rank methods\ngmm_pca = fit(PCAEM(3, 10), high_dim_data)\ngmm_factor = fit(FactorEM(3, 10), high_dim_data)","category":"page"},{"location":"fitting/#Related-Documentation","page":"Fitting","title":"Related Documentation","text":"","category":"section"},{"location":"fitting/","page":"Fitting","title":"Fitting","text":"Structured Gaussians: Learn about the low-rank plus diagonal distribution used by PCAEM and FactorEM methods ","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page provides simple working examples demonstrating typical workflows for fitting, prediction, and weighted fitting.","category":"page"},{"location":"examples/#Example-1:-Basic-Fitting","page":"Examples","title":"Example 1: Basic Fitting","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to fit GMMs using the three different methods.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using StructuredGaussianMixtures\n\n# Generate some data\ndata = randn(2, 1000)\n\n# Fit with different methods\ngmm_em = fit(EM(3), data)\ngmm_pca = fit(PCAEM(3, 1), data)\ngmm_factor = fit(FactorEM(3, 1), data)\n\n# Evaluate performance\nprintln(\"EM log-likelihood: \", mean(logpdf(gmm_em, data)))\nprintln(\"PCAEM log-likelihood: \", mean(logpdf(gmm_pca, data)))\nprintln(\"FactorEM log-likelihood: \", mean(logpdf(gmm_factor, data)))","category":"page"},{"location":"examples/#Example-2:-High-Dimensional-Data","page":"Examples","title":"Example 2: High-Dimensional Data","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example demonstrates the effectiveness of low-rank methods for high-dimensional data.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# High-dimensional data\nn_features = 100\nn_samples = 50\ndata = randn(n_features, n_samples)\n\n# Compare methods\nprintln(\"Fitting EM model...\")\n@time gmm_em = fit(EM(3), data)\n\nprintln(\"Fitting PCAEM model...\")\n@time gmm_pca = fit(PCAEM(3, 10), data)\n\nprintln(\"Fitting FactorEM model...\")\n@time gmm_factor = fit(FactorEM(3, 10), data)\n\n# Compare performance\nprintln(\"EM log-likelihood: \", mean(logpdf(gmm_em, data)))\nprintln(\"PCAEM log-likelihood: \", mean(logpdf(gmm_pca, data)))\nprintln(\"FactorEM log-likelihood: \", mean(logpdf(gmm_factor, data)))","category":"page"},{"location":"examples/#Example-3:-Conditional-Prediction","page":"Examples","title":"Example 3: Conditional Prediction","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example demonstrates how to perform conditional prediction.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Make prediction\nx_query = [0.5]\nposterior = predict(gmm, x_query)\n\n# Generate samples from posterior\nsamples = rand(posterior, 100)\nprintln(\"Posterior mean: \", mean(samples))\nprintln(\"Posterior variance: \", var(samples))","category":"page"},{"location":"examples/#Example-4:-Weighted-Fitting","page":"Examples","title":"Example 4: Weighted Fitting","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to fit a GMM with weighted data points.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Generate data\ndata = randn(2, 1000)\n\n# Create weights based on data values\nweights = [data[1, i] > 0 ? 1.0 : 0.5 for i in 1:size(data, 2)]\n\n# Fit with weights (only FactorEM supports this)\ngmm_weighted = fit(FactorEM(3, 1), data, weights)\n\n# Print results\nprintln(\"Number of samples with weight 1: \", sum(weights .== 1.0))\nprintln(\"Number of samples with weight 0.5: \", sum(weights .== 0.5))\nprintln(\"Weighted log-likelihood: \", mean(logpdf(gmm_weighted, data)))","category":"page"},{"location":"examples/#Example-5:-LRDMvNormal-Usage","page":"Examples","title":"Example 5: LRDMvNormal Usage","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to work directly with LRDMvNormal distributions.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using StructuredGaussianMixtures\n\n# Create a low-rank distribution\nn_features = 10\nrank = 3\nμ = randn(n_features)\nF = randn(n_features, rank)\nD = ones(n_features) * 0.1\n\ndist = LRDMvNormal(μ, F, D)\n\n# Basic properties\nprintln(\"Dimension: \", length(dist))\nprintln(\"Rank: \", rank(dist))\nprintln(\"Mean: \", mean(dist)[1:5])  # First 5 elements\n\n# Generate samples\nsamples = rand(dist, 1000)\nprintln(\"Sample shape: \", size(samples))\n\n# Compute log probability\nlog_prob = logpdf(dist, samples[:, 1])\nprintln(\"Log probability: \", log_prob)","category":"page"},{"location":"examples/#Example-6:-Partial-Prediction","page":"Examples","title":"Example 6: Partial Prediction","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to predict specific dimensions.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Fit a 5D GMM\ndata = randn(5, 1000)\ngmm = fit(EM(3), data)\n\n# Observe dimensions 1 and 3, predict dimensions 2, 4, and 5\nobserved_values = [0.5, -0.2]\nobserved_dims = [1, 3]\noutput_dims = [2, 4, 5]\n\nposterior = predict(gmm, observed_values; \n    input_indices=observed_dims, \n    output_indices=output_dims)\n\n# Generate samples\nsamples = rand(posterior, 100)\nprintln(\"Predicted dimensions shape: \", size(samples))","category":"page"},{"location":"examples/#Example-7:-Multiple-Query-Points","page":"Examples","title":"Example 7: Multiple Query Points","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This example shows how to make predictions for multiple query points.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Predict for multiple query points\nquery_points = [[0.5], [-0.2], [1.1]]\nposteriors = [predict(gmm, q) for q in query_points]\n\n# Generate samples for each\nsamples = [rand(p, 50) for p in posteriors]\n\n# Print results\nfor (i, (q, s)) in enumerate(zip(query_points, samples))\n    println(\"Query $i: mean = \", mean(s), \", variance = \", var(s))\nend","category":"page"},{"location":"examples/#Running-the-Examples","page":"Examples","title":"Running the Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"To run these examples:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Install dependencies:\nusing Pkg\nPkg.add([\"StructuredGaussianMixtures\", \"Distributions\", \"LinearAlgebra\", \"Random\", \"Statistics\"])\nLoad the package:\nusing StructuredGaussianMixtures\nRun examples: Copy and paste any example into a Julia session.","category":"page"},{"location":"prediction/#Prediction","page":"Prediction","title":"Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"This page describes how to perform conditional prediction with fitted Gaussian Mixture Models using StructuredGaussianMixtures.jl.","category":"page"},{"location":"prediction/#Overview","page":"Prediction","title":"Overview","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"The predict function computes posterior distributions over unobserved dimensions given observed values. This is useful for:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"Missing data imputation: Fill in missing values based on observed data\nConditional sampling: Generate samples from the posterior distribution\nUncertainty quantification: Assess uncertainty in predictions","category":"page"},{"location":"prediction/#Function-Signature","page":"Prediction","title":"Function Signature","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"predict(gmm, x; input_indices=1:length(x), output_indices=length(x)+1:length(gmm))","category":"page"},{"location":"prediction/#Parameters","page":"Prediction","title":"Parameters","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"gmm: A fitted MixtureModel\nx: Observed values\ninput_indices: Indices of observed dimensions (default: 1:length(x))\noutput_indices: Indices of dimensions to predict (default: length(x)+1:length(gmm))","category":"page"},{"location":"prediction/#Basic-Usage","page":"Prediction","title":"Basic Usage","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"using StructuredGaussianMixtures\n\n# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Make predictions\nquery_point = [0.5]  # Observed value for first dimension\nposterior = predict(gmm, query_point)  # Posterior over second dimension","category":"page"},{"location":"prediction/#Predict-Functions","page":"Prediction","title":"Predict Functions","text":"","category":"section"},{"location":"prediction/#MvNormal-Prediction","page":"Prediction","title":"MvNormal Prediction","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{MvNormal, AbstractVector, Union{AbstractRange, Vector{Int64}}, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::MvNormal, x::AbstractVector, input_indices::Union{Vector{Int},AbstractRange}, output_indices::Union{Vector{Int},AbstractRange})\n\nCompute the conditional distribution of the output indices given the input indices using the Schur complement. Returns a new MvNormal distribution representing the conditional distribution.\n\nArguments\n\ndist: The multivariate normal distribution\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables\noutput_indices: The indices of the variables to predict\n\nReturns\n\nA new MvNormal distribution representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#LRDMvNormal-Prediction","page":"Prediction","title":"LRDMvNormal Prediction","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{LRDMvNormal, AbstractVector, Union{AbstractRange, Vector{Int64}}, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::LRDMvNormal, x::AbstractVector, input_indices::Union{Vector{Int},AbstractRange}, output_indices::Union{Vector{Int},AbstractRange})\n\nCompute the conditional distribution of the output indices given the input indices using the Schur complement. Returns a new LRDMvNormal distribution representing the conditional distribution. This implementation is efficient for low-rank plus diagonal covariance structure.\n\nArguments\n\ndist: The low-rank plus diagonal multivariate normal distribution\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables\noutput_indices: The indices of the variables to predict\n\nReturns\n\nA new LRDMvNormal distribution representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#MixtureModel-Prediction","page":"Prediction","title":"MixtureModel Prediction","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{MultivariateMixture, AbstractVector, Union{AbstractRange, Vector{Int64}}, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::MultivariateMixture, x::AbstractVector, input_indices::Union{Vector{Int},AbstractRange}, output_indices::Union{Vector{Int},AbstractRange})\n\nCompute the conditional distribution of the output indices given the input indices for a mixture model. Returns a new mixture model where each component is the conditional distribution of the corresponding component, and the weights are updated based on the log density of x under the marginal distributions.\n\nArguments\n\ndist: The multivariate mixture distribution\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables\noutput_indices: The indices of the variables to predict\n\nReturns\n\nA new mixture model representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#Convenience-Function","page":"Prediction","title":"Convenience Function","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.predict-Tuple{Union{LRDMvNormal, MvNormal, MultivariateMixture}, AbstractVector}","page":"Prediction","title":"StructuredGaussianMixtures.predict","text":"predict(dist::Union{MvNormal,LRDMvNormal,MultivariateMixture}, x::AbstractVector; \n       input_indices::Union{Vector{Int},AbstractRange} = 1:length(x), \n       output_indices::Union{Vector{Int},AbstractRange} = length(x)+1:length(mean(dist)))\n\nCompute the conditional distribution of the output indices given the input indices using the Schur complement. Returns a new distribution representing the conditional distribution.\n\nArguments\n\ndist: The multivariate normal distribution (MvNormal or LRDMvNormal)\nx: The observed values for the input indices\ninput_indices: The indices of the observed variables (default: first length(x) indices)\noutput_indices: The indices of the variables to predict (default: remaining indices)\n\nReturns\n\nA new distribution representing the conditional distribution\n\n\n\n\n\n","category":"method"},{"location":"prediction/#Marginal-Functions","page":"Prediction","title":"Marginal Functions","text":"","category":"section"},{"location":"prediction/#Marginal-Distribution","page":"Prediction","title":"Marginal Distribution","text":"","category":"section"},{"location":"prediction/#StructuredGaussianMixtures.marginal-Tuple{MvNormal, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.marginal","text":"marginal(dist::MvNormal, indices::Union{Vector{Int},AbstractRange})\n\nCompute the marginal distribution over the specified indices. Returns a new MvNormal distribution representing the marginal.\n\nArguments\n\ndist: The multivariate normal distribution\nindices: The indices to marginalize over\n\nReturns\n\nA new MvNormal distribution representing the marginal\n\n\n\n\n\n","category":"method"},{"location":"prediction/#StructuredGaussianMixtures.marginal-Tuple{LRDMvNormal, Union{AbstractRange, Vector{Int64}}}","page":"Prediction","title":"StructuredGaussianMixtures.marginal","text":"marginal(dist::LRDMvNormal, indices::Union{Vector{Int},AbstractRange})\n\nCompute the marginal distribution over the specified indices. Returns a new LRDMvNormal distribution representing the marginal.\n\nArguments\n\ndist: The low-rank plus diagonal multivariate normal distribution\nindices: The indices to marginalize over\n\nReturns\n\nA new LRDMvNormal distribution representing the marginal\n\n\n\n\n\n","category":"method"},{"location":"prediction/#Simple-Examples","page":"Prediction","title":"Simple Examples","text":"","category":"section"},{"location":"prediction/#2D-GMM-Prediction","page":"Prediction","title":"2D GMM Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Fit a GMM\ndata = randn(2, 1000)\ngmm = fit(EM(3), data)\n\n# Make prediction\nx_query = [0.5]\nposterior = predict(gmm, x_query)\n\n# Generate samples from posterior\nsamples = rand(posterior, 100)\nprintln(\"Posterior mean: \", mean(samples))","category":"page"},{"location":"prediction/#High-Dimensional-Prediction","page":"Prediction","title":"High-Dimensional Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Fit a high-dimensional GMM\ndata = randn(10, 1000)\ngmm = fit(PCAEM(3, 3), data)\n\n# Predict multiple dimensions\nobserved_dims = [1, 3, 5]\nquery_values = [0.5, -0.2, 1.1]\nunobserved_dims = [2, 4, 6, 7, 8, 9, 10]\n\nposterior = predict(gmm, query_values; \n    input_indices=observed_dims, \n    output_indices=unobserved_dims)\n\n# Generate samples\nsamples = rand(posterior, 100)\nprintln(\"Predicted dimensions shape: \", size(samples))","category":"page"},{"location":"prediction/#Partial-Prediction","page":"Prediction","title":"Partial Prediction","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Fit a 5D GMM\ndata = randn(5, 1000)\ngmm = fit(EM(3), data)\n\n# Observe dimensions 1 and 3, predict dimensions 2, 4, and 5\nobserved_values = [0.5, -0.2]\nobserved_dims = [1, 3]\noutput_dims = [2, 4, 5]\n\nposterior = predict(gmm, observed_values; \n    input_indices=observed_dims, \n    output_indices=output_dims)\n\nsamples = rand(posterior, 100)\nprintln(\"Predicted dimensions shape: \", size(samples))","category":"page"},{"location":"prediction/#Mathematical-Background","page":"Prediction","title":"Mathematical Background","text":"","category":"section"},{"location":"prediction/#Conditional-Gaussian-Distribution","page":"Prediction","title":"Conditional Gaussian Distribution","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"For a Gaussian Mixture Model with components k = 1 ldots K, the posterior distribution given observed values x_obs is:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"p(x_unobs mid x_obs) = sum_k=1^K w^k mathcalN(x_unobs mid mu^k_unobs Sigma^k_unobs)","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"where w^k are the posterior component weights, mu^k_unobs is the conditional mean for component k, and Sigma^k_unobs is the conditional covariance for component k","category":"page"},{"location":"prediction/#Component-Weight-Updates","page":"Prediction","title":"Component Weight Updates","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"The posterior component weights are computed as:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"w^k = fracpi^k mathcalN(x_obs mid mu^k_obs Sigma^k_obs)sum_j=1^K pi^j mathcalN(x_obs mid mu^j_obs Sigma^j_obs)","category":"page"},{"location":"prediction/#Conditional-Parameters","page":"Prediction","title":"Conditional Parameters","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"For each component k, the conditional parameters are:","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"mu^k_unobs = mu^k_unobs + Sigma^k_unobsobs (Sigma^k_obs)^-1 (x_obs - mu^k_obs)","category":"page"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"Sigma^k_unobs = Sigma^k_unobsunobs - Sigma^k_unobsobs (Sigma^k_obs)^-1 Sigma^k_obsunobs","category":"page"},{"location":"prediction/#Advanced-Usage","page":"Prediction","title":"Advanced Usage","text":"","category":"section"},{"location":"prediction/#Multiple-Query-Points","page":"Prediction","title":"Multiple Query Points","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Predict for multiple query points\nquery_points = [[0.5], [-0.2], [1.1]]\nposteriors = [predict(gmm, q) for q in query_points]\n\n# Generate samples for each\nsamples = [rand(p, 50) for p in posteriors]","category":"page"},{"location":"prediction/#Uncertainty-Quantification","page":"Prediction","title":"Uncertainty Quantification","text":"","category":"section"},{"location":"prediction/","page":"Prediction","title":"Prediction","text":"# Compute posterior statistics\nposterior = predict(gmm, query_point)\nsamples = rand(posterior, 1000)\n\n# Mean and variance\nposterior_mean = mean(samples, dims=2)\nposterior_var = var(samples, dims=2)\n\n# Confidence intervals\nposterior_quantiles = quantile(samples, [0.025, 0.975], dims=2)","category":"page"},{"location":"#StructuredGaussianMixtures.jl","page":"Home","title":"StructuredGaussianMixtures.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for fitting and conditional prediction of Gaussian Mixture Models (GMMs) with structured covariance matrices.","category":"page"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Efficient high-dimensional fitting: Different fitting methods, including those that efficiently fit low-rank covariance structures for m ≫ n settings\nConditional prediction: Compute posterior distributions over unobserved variables\nWeighted fitting: Support for weighted data points in model fitting","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StructuredGaussianMixtures\")","category":"page"},{"location":"#Methods","page":"Home","title":"Methods","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements three main fitting methods for Gaussian Mixture Models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"FactorEM uses EM to fit a GMM with low-rank-plus-diagonal covariance structure Sigma = FF + D, using an inner EM step to update the covariance components. This is the only method that currently supports weighted fitting\nEM uses standard EM to fit a GMM with full rank convariance\nPCAEM fits a GMM with low-rank-plus-diagonal covariance structure by fitting a full-rank GMM on PCA-compressed data","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using StructuredGaussianMixtures\n\n# Fit a GMM using EM\ndata = randn(100, 1000)  # 100D data with 1000 samples\nw = rand(1000) # weights on data \ngmm = fit(FactorEM(3,5), data, w)  # 3-component rank-5 low-rank-plus-diagonal GMM\n\n# Make predictions\nquery_point = [0.5]\nposterior = predict(gmm, query_point)  # Posterior over dimensions 2:100 when x1 = 0.5","category":"page"},{"location":"#Documentation-and-API-Reference-sections","page":"Home","title":"Documentation and API Reference sections","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Fitting Methods: Learn about the different fitting algorithms and when to use each\nStructured Gaussians: Learn about the structured Gaussians underpinning this project  \nPrediction: Understand conditional prediction and posterior computation\nExamples: Complete working examples from the test files","category":"page"},{"location":"lrdmvnormal/#Structured-Gaussians","page":"Structured Gaussians","title":"Structured Gaussians","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"This page documents the LRDMvNormal class, which represents low-rank plus diagonal multivariate normal distributions.","category":"page"},{"location":"lrdmvnormal/#Overview","page":"Structured Gaussians","title":"Overview","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"The LRDMvNormal distribution represents a multivariate normal distribution with covariance matrix of the form Sigma = FF + D, where:","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"F is a low-rank factor matrix of size m times r where r ll m\nD is a diagonal matrix\nThe full covariance matrix is never explicitly formed for efficiency","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"This structure is particularly useful in the following circumstances:","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"High-dimensional data where m gg n (number of features much larger than number of samples)\nLow-rank structure in the data\nMemory constraints preventing full covariance storage\nEfficient sampling requirements","category":"page"},{"location":"lrdmvnormal/#Constructor","page":"Structured Gaussians","title":"Constructor","text":"","category":"section"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.LRDMvNormal","page":"Structured Gaussians","title":"StructuredGaussianMixtures.LRDMvNormal","text":"LRDMvNormal\n\nLow-rank plus diagonal multivariate normal distribution. The covariance matrix is represented as Σ = FF' + D, where F is a low-rank factor matrix and D is a diagonal matrix.\n\nFields\n\nμ: Mean vector\nF: Low-rank factor matrix\nD: Diagonal vector\nrank: Rank of the low-rank component\n\nNotes\n\nThe covariance matrix is never explicitly formed\nAll operations use the low-rank plus diagonal structure for efficiency\n\n\n\n\n\n","category":"type"},{"location":"lrdmvnormal/#Usage","page":"Structured Gaussians","title":"Usage","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"using StructuredGaussianMixtures\n\n# Create a 10-dimensional distribution with rank-3 structure\nμ = randn(10)           # Mean vector\nF = randn(10, 3)        # Low-rank factor (10×3)\nD = ones(10)            # Diagonal vector\ndist = LRDMvNormal(μ, F, D)","category":"page"},{"location":"lrdmvnormal/#Distribution-Interface","page":"Structured Gaussians","title":"Distribution Interface","text":"","category":"section"},{"location":"lrdmvnormal/#Basic-Properties","page":"Structured Gaussians","title":"Basic Properties","text":"","category":"section"},{"location":"lrdmvnormal/#Base.length-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"Base.length","text":"length(d::LRDMvNormal)\n\nReturn the dimension of the distribution.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Base.size-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"Base.size","text":"size(d::LRDMvNormal)\n\nReturn the size of the distribution as a tuple (dimension,).\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Statistics.mean-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"Statistics.mean","text":"mean(d::LRDMvNormal)\n\nReturn the mean vector of the distribution.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Statistics.cov-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"Statistics.cov","text":"cov(d::LRDMvNormal)\n\nReturn the full covariance matrix FF' + D.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Low-Rank-Structure-Access","page":"Structured Gaussians","title":"Low-Rank Structure Access","text":"","category":"section"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.rank-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"StructuredGaussianMixtures.rank","text":"rank(d::LRDMvNormal)\n\nReturn the rank of the low-rank component.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.low_rank_factor-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"StructuredGaussianMixtures.low_rank_factor","text":"low_rank_factor(d::LRDMvNormal)\n\nReturn the low-rank factor matrix F.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#StructuredGaussianMixtures.diagonal-Tuple{LRDMvNormal}","page":"Structured Gaussians","title":"StructuredGaussianMixtures.diagonal","text":"diagonal(d::LRDMvNormal)\n\nReturn the diagonal vector D.\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Probability-and-Sampling","page":"Structured Gaussians","title":"Probability and Sampling","text":"","category":"section"},{"location":"lrdmvnormal/#Log-Probability-Density","page":"Structured Gaussians","title":"Log Probability Density","text":"","category":"section"},{"location":"lrdmvnormal/#Distributions.logpdf-Tuple{LRDMvNormal, AbstractVector}","page":"Structured Gaussians","title":"Distributions.logpdf","text":"logpdf(d::LRDMvNormal, x::AbstractVector)\n\nCompute the log probability density function at x. Uses the matrix inversion lemma for efficient computation.\n\nArguments\n\nd: The LRDMvNormal distribution\nx: The point at which to evaluate the log PDF\n\nReturns\n\nThe log probability density at x\n\nNotes\n\nUses the matrix inversion lemma: (FF' + D)^(-1) = D^(-1) - D^(-1)F(I + F'D^(-1)F)^(-1)F'*D^(-1)\nComputes the determinant efficiently: det(FF' + D) = det(D) * det(I + F'D^(-1)*F)\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Random-Sampling","page":"Structured Gaussians","title":"Random Sampling","text":"","category":"section"},{"location":"lrdmvnormal/#Distributions._rand!-Tuple{AbstractRNG, LRDMvNormal, VecOrMat}","page":"Structured Gaussians","title":"Distributions._rand!","text":"_rand!(rng::AbstractRNG, d::LRDMvNormal, x::VecOrMat)\n\nGenerate random samples in-place from the distribution.\n\nArguments\n\nrng: Random number generator\nd: The LRDMvNormal distribution\nx: Vector or matrix to fill with random samples\n\nReturns\n\nThe filled vector/matrix x\n\nNotes\n\nUses the decomposition: X = μ + FZ₁ + sqrt(D)Z₂ where Z₁, Z₂ are standard normal\nFor matrices, each column is a sample\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Distributions._rand!-Tuple{AbstractRNG, LRDMvNormal, AbstractVector}","page":"Structured Gaussians","title":"Distributions._rand!","text":"_rand!(rng::AbstractRNG, d::LRDMvNormal, x::AbstractVector)\n\nGenerate a random sample in-place from the distribution.\n\nArguments\n\nrng: Random number generator\nd: The LRDMvNormal distribution\nx: Vector to fill with random sample\n\nReturns\n\nThe filled vector x\n\nNotes\n\nUses the decomposition: X = μ + FZ₁ + sqrt(D)Z₂ where Z₁, Z₂ are standard normal\nHandles AbstractVector types that don't support randn!\n\n\n\n\n\n","category":"method"},{"location":"lrdmvnormal/#Simple-Examples","page":"Structured Gaussians","title":"Simple Examples","text":"","category":"section"},{"location":"lrdmvnormal/#Creating-and-Using-LRDMvNormal","page":"Structured Gaussians","title":"Creating and Using LRDMvNormal","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"using StructuredGaussianMixtures\n\n# Create a low-rank distribution\nn_features = 100\nrank = 5\nμ = randn(n_features)\nF = randn(n_features, rank)\nD = ones(n_features) * 0.1\n\ndist = LRDMvNormal(μ, F, D)\n\n# Basic properties\nprintln(\"Dimension: \", length(dist))\nprintln(\"Rank: \", rank(dist))\nprintln(\"Mean: \", mean(dist)[1:5])  # First 5 elements\n\n# Generate samples\nsamples = rand(dist, 1000)\nprintln(\"Sample shape: \", size(samples))\n\n# Compute log probability\nlog_prob = logpdf(dist, samples[:, 1])\nprintln(\"Log probability: \", log_prob)","category":"page"},{"location":"lrdmvnormal/#Efficient-Operations","page":"Structured Gaussians","title":"Efficient Operations","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"# The covariance matrix is never explicitly formed\n# This is efficient even for high dimensions\nn_features = 1000\nrank = 10\n\nμ = randn(n_features)\nF = randn(n_features, rank)\nD = ones(n_features) * 0.1\n\ndist = LRDMvNormal(μ, F, D)\n\n# This is efficient - no O(n³) operations\nlog_prob = logpdf(dist, randn(n_features))\n\n# Sampling is also efficient\nsamples = rand(dist, 100)","category":"page"},{"location":"lrdmvnormal/#Accessing-Components","page":"Structured Gaussians","title":"Accessing Components","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"# Get the low-rank structure\nF_matrix = low_rank_factor(dist)\nD_vector = diagonal(dist)\nrank_val = rank(dist)\n\nprintln(\"F shape: \", size(F_matrix))\nprintln(\"D length: \", length(D_vector))\nprintln(\"Rank: \", rank_val)\n\n# Full covariance (only for small dimensions!)\nfull_cov = cov(dist)\nprintln(\"Full covariance shape: \", size(full_cov))","category":"page"},{"location":"lrdmvnormal/#Mathematical-Background","page":"Structured Gaussians","title":"Mathematical Background","text":"","category":"section"},{"location":"lrdmvnormal/#Efficient-Log-Likelihood-Computation","page":"Structured Gaussians","title":"Efficient Log-Likelihood Computation","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"The log probability density is computed efficiently using block elimination and the matrix inversion lemma to compute the quadratic form, and the standard determinant formula for low-rank plus diagonal matrices. The approach avoids forming the full covariance matrix and reduces computational complexity from O(m³) to O(mr² + r³) where r ≪ m.","category":"page"},{"location":"lrdmvnormal/#Sampling","page":"Structured Gaussians","title":"Sampling","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"Samples are generated using the decomposition:","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"X = mu + FZ_1 + sqrtDZ_2","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"where Z_1 and Z_2 are independent standard normal random vectors.","category":"page"},{"location":"lrdmvnormal/#Performance-Characteristics","page":"Structured Gaussians","title":"Performance Characteristics","text":"","category":"section"},{"location":"lrdmvnormal/#Computational-Complexity","page":"Structured Gaussians","title":"Computational Complexity","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"Operation Complexity Notes\nLog-likelihood O(mr² + r³) Uses block elimination with Schur complement\nSampling O(mr) Efficient decomposition\nMemory O(mr) Stores F and D, not full covariance","category":"page"},{"location":"lrdmvnormal/#Memory-Usage","page":"Structured Gaussians","title":"Memory Usage","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"For a distribution with m features and rank r:","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"Storage: O(mr + m) = O(mr) for F and D\nTraditional: O(m²) for full covariance matrix\nSavings: O(m²mr) = O(mr) for typical r ll m","category":"page"},{"location":"lrdmvnormal/#Integration-with-GMMs","page":"Structured Gaussians","title":"Integration with GMMs","text":"","category":"section"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"The LRDMvNormal distribution is used internally by PCAEM and FactorEM methods:","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"# PCAEM creates LRDMvNormal components\ngmm = fit(PCAEM(3, 5), data)\nfor comp in components(gmm)\n    println(\"Component rank: \", rank(comp))\nend\n\n# FactorEM also creates LRDMvNormal components\ngmm = fit(FactorEM(3, 5), data)\nfor comp in components(gmm)\n    println(\"Component rank: \", rank(comp))\nend","category":"page"},{"location":"lrdmvnormal/","page":"Structured Gaussians","title":"Structured Gaussians","text":"","category":"page"}]
}
